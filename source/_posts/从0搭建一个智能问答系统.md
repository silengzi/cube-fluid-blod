---
title: 从0搭建一个智能问答系统
date: 2025-07-18T01:48:00.000Z
tags: 
  - python
  - poetry
  - FastAPI
  - LangChain
  - 工程化
  - 项目初始化
  - 智能问答

---


摘要...


## 技术选型

| 模块       | 技术栈                     |
|------------|----------------------------|
| Python 版本 | `pyenv` + `poetry` 管理   |
| 虚拟环境   | `poetry` 自动隔离         |
| Web 框架   | `FastAPI` + `uvicorn`     |
| AI 问答    | `LangChain` + `OpenAI SDK`|

---

## 项目结构

```txt
  ai-qa-bot/
  ├── app/
  │ ├── main.py # FastAPI 服务入口
  │ ├── api/
  │ │ └── chat.py # /chat 接口定义
  │ ├── services/
  │ │ └── qa_service.py # LangChain 封装
  │ └── models/
  │ └── message.py # 请求/响应模型
  ├── tests/ # 测试代码
  ├── .env # OpenAI 密钥
  ├── pyproject.toml # Poetry 配置
  └── README.md # 项目文档（本文件）
```

---

## 配置环境

使用 `pyenv` 管理 python 版本。

```bash
pyenv install 3.11.9
pyenv global 3.11.9
```

---

## 项目初始化

#### 1. 安装 poetry ( windows )

使用 `poetry` 创建和管理项目，比如虚拟环境、安装依赖等。
1. 打开 PowerShell
2. 执行以下命令：
```powershell
(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -
```
要求：系统已安装 Python，且 python 命令可用

#### 2. 配置环境变量

使用 `poetry --version` 验证是否安装成功，如未成功，则需要手动配置环境变量。
Poetry 安装在用户目录下，默认路径是：
```
C:\Users\<你的用户名>\AppData\Roaming\Python\Scripts
```


创建项目 & 安装依赖

---

## 启动 FastAPI 服务

```bash
poetry run uvicorn app.main:app --reload
```

访问接口文档：
http://127.0.0.1:8000/docs

---

## 示例 API：流式问答接口

#### POST /chat

```json
请求体：
{
  "message": "你好，请介绍你自己"
}
```
返回类型：StreamingResponse（逐步返回回答）

---

## 核心代码说明

#### app/main.py
```python
from fastapi import FastAPI
from app.api import chat
from dotenv import load_dotenv

load_dotenv()
app = FastAPI()
app.include_router(chat.router)
```

#### app/api/chat.py

```python
from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse
from app.services.qa_service import get_chat_response
from app.models.message import ChatRequest

router = APIRouter()

@router.post("/chat")
async def chat(req: Request):
    data = await req.json()
    prompt = data.get("message", "")
    
    def stream():
        for chunk in get_chat_response(prompt):
            yield chunk
    
    return StreamingResponse(stream(), media_type="text/plain")
```

#### app/services/qa_service.py

```python
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
import os

llm = ChatOpenAI(
    temperature=0.7,
    streaming=False,
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory)

def get_chat_response(prompt: str):
    yield "🤖 正在思考...\n"
    response = conversation.predict(input=prompt)
    yield response
```

#### app/models/message.py

```python
from pydantic import BaseModel

class ChatRequest(BaseModel):
    message: str
```




![加油](https://silengzi.github.io/cube-fluid-blod/images/006APoFYly8h5mmvdv4zyg307n07nu10.gif)